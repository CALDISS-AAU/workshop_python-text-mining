{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ustruktureret data og text mining i Python\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Warning.svg/1200px-Warning.svg.png\" width = 400>\n",
    "\n",
    "I dag gives en generel introduktion til web scraping. I vil med disse v√¶rkt√∏jer kunne g√• direkte i gang med at indsamle data fra forskellige hjemmesider.\n",
    "\n",
    "Legaliteten af web scraping er lidt af en gr√•zone, s√• man skal t√¶nke sig om, inden man s√¶tter scraping v√¶rkt√∏jer i gang, da man nemt kan komme til at g√∏re ting, der er i strid med ophavsretsloven eller persondataloven.\n",
    "\n",
    "### Web scraping og ophavsret/brugsbetingelser\n",
    "De data, som virksomheder, organisationer o.l., l√¶gger p√• hjemmesider, er ejet af disse virksomheder og organisationer. Mange hjemmesider har brugsbetingelser, der forbyder brug af scraping p√• deres hjemmesider. Dette blandt andet fordi, at man med scraping i princippet kan opstille en hjemmeside, der kopierer indhold direkte fra andre sider. Selvom det ikke er vores hensigt med den type scraping, som vi laver, s√• vil det stadig blive betragtet som et brud.\n",
    "\n",
    "### Web scraping og persondata\n",
    "Data p√• sociale medier er en gevaldig gr√•zone persondata-m√¶ssigt: Er data, som personer frivilligt l√¶gger offentligt p√• deres sociale medier, stadig deres data? I nogen tilf√¶lde er svaret \"ja\", I andre er det \"nej\".\n",
    "Derudover skal man v√¶re opm√¶rksom p√•, at sider som Facebook, Twitter og Instagram ogs√• har betingelser skrevet ind for brug af deres data, da de er dataansvarlige for de data, som l√¶gges op. Ved Facebook og Instagram er det fx ikke tilladt at automatisk indhente deres data.\n",
    "\n",
    "### Web scraping og \"hacking\"\n",
    "En hjemmeside befinder sig p√• en server. Hver gang en hjemmeside bes√∏ges, modtager serveren en henvendelse. Jo flere henvendelser, jo mere belastet bliver serveren. \n",
    "Fordi vi med Python meget nemt kan skrive kode, der gentager en kommando, kan man sende utroligt mange henvendelser p√• meget kort tid. \n",
    "\n",
    "***Dette vil for det meste bliver betragtet som et angreb og er ulovligt!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hvad er ustruktureret data?\n",
    "\n",
    "*Ustruktureret data* er data, som - kort sagt - ikke er sat i system. Hvis data ikke er struktureret, vil man typisk kalde det ustruktureret data. Tekst, billeder og video er typiske eksempler p√• ustruktureret data, da disse blot er r√• information uden nogen m√•de at adskille en type information fra en anden. Mange moderne dataanalyseteknikker fokuserer p√• ustruktureret data, hvor man enten udvikler teknikker til at skabe overblik over ustruktureret data eller fors√∏ger at give data en form for struktur.\n",
    "\n",
    "Nedenst√•ende er et eksempel p√• ustruktureret data:\n",
    "\n",
    "```\n",
    "[\"Hvorfor g√•r man ikke i dialog med ‚Å¶@DRC_dk‚Å© i stedet for at opsige kontrakten uden varsel. Er det kun for at f√∏re st√¶rk mand politik? DRC yder en fremragende indsats p√• baggrund af den opgave de har f√•et #dkpol https://jyllands-posten.dk/indland/ECE12248020/tesfaye-forsoeger-sig-med-en-ny-loesning-paa-alle-udlaendingeministres-problem/ ‚Ä¶\",\n",
    "\"Alle t√¶ller ‚ù§Ô∏è https://twitter.com/cekicozlem/status/1276034922587832326 ‚Ä¶\",\n",
    "\"Det er s√• godt arbejdeüíö https://twitter.com/fannybroholm/status/1275360842847080449 ‚Ä¶\",\n",
    "\"Tilfreds med den klima og energiaftale, der er lavet nu. Det er den f√∏rste delaftale om at n√• 70% reduktion i 2030. S√¶rligt glad for at den indeholder principaftale om en CO2 afgiftsreform #dkpol #dkgreen pic.twitter.com/3slrMxLT5B\",\n",
    "\"Godt f√∏rste skridt for den fri natur #dkpol #dkgreen ‚Å¶@alternativet_‚Å© https://www.altinget.dk/miljoe/artikel/wermelin-lander-aftale-om-de-foerste-naturnationalparker ‚Ä¶\",\n",
    "\"Sp√¶ndende udmelding. ‚Å¶@alternativet_‚Å© √∏nsker ogs√• en gr√∏n   Klimaafgift, hvor udgangspunktet er at forureneren betaler #dkgreen #dkpol https://www.altinget.dk/artikel/venstre-og-radikale-laegger-faelles-pres-paa-regeringen-vil-have-ensartet-co2-afgift?SNSubscribed=true&ref=newsletter&refid=fredag-middag-190620&utm_campaign=altingetdk%20Altinget.dk&utm_medium%09=e-mail&utm_source=nyhedsbrev ‚Ä¶\",\n",
    "\"S√• vigtigt at KL tager ansvar for den proces #dkpol #dkgreen https://www.altinget.dk/miljoe/artikel/professor-om-affaldsaftale-kl-og-kommunerne-skal-gribe-chancen-for-at-loese-tingene-selv ‚Ä¶\",\n",
    "\"Hurra - stor dag for Danmarküíöüëèüèºüëèüèº https://twitter.com/alternativet_/status/1273555055476723713 ‚Ä¶\",\n",
    "\"Til klimaforhandlinger i Finansministeriet. Vi sidder og diskuterer rammerne - de n√¶ste dage bliver intensive #dkpol #dkgreen @alternativet_ @ Christiansborg Palace  https://www.instagram.com/p/CBi3d0oB9lB/?igshid=ii78cjnx2n72 ‚Ä¶\",\n",
    "\"Aftale om mindre affald, mindre forbr√¶nding og mere genbrug - god dag for klimaet og milj√∏et. 1. skridt i en stor milj√∏pakke #dkpol ‚Å¶@alternativet_‚Å© https://www.dr.dk/nyheder/indland/live-regeringen-praesenterer-ny-aftale-om-affald ‚Ä¶\"]\n",
    "```\n",
    "\n",
    "I ovenst√•ende kan man stadig overf√∏re ideen om observationer (i dette tilf√¶lde tweets fra danske politikere), men der er ingen variable eller features givet p√• forh√•nd. Der er derfor ingen umiddelbar struktur at g√∏re brug af for at foretage sig analyser.\n",
    "\n",
    "## Hvorfor er ustruktureret data interessant?\n",
    "\n",
    "En rigtig stor del af det data, som produceres, er ustruktureret. T√¶nk p√• hvor meget data, der produceres p√• hjemmesider, sociale medier o.l. Det meste af dette er i form af tekst; alts√• ustruktureret. \n",
    "\n",
    "\n",
    "# Text mining og NLP\n",
    "\n",
    "Fordi s√• meget data er i form af ustruktureret tekst, er der brug for metoder og teknikker til at behandle disse. Disse teknikker og metoder omtales overordnet som \"text mining\" og \"natural language processing\" (NLP). De to termer bruges lidt synonymt. \"Text mining\" er mest et dataanalyse term, mens NLP er en gren af computervidenskab. Groft sagt er forskellen, at NLP er videnskaben/metoder til at f√• en computer til at forst√• tekst, mens \"text mining\" er analysev√¶rkt√∏jerne til at udlede m√∏nstre i tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer spacy's danske sprogmodel\n",
    "#! python -m spacy download da_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datah√•ndtering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import string\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "# Web scraping\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "# Visualisering\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# S√¶t visualiseringsindstillinger\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10,6)})\n",
    "\n",
    "# Indl√¶s sprogmodel\n",
    "nlp = spacy.load(\"da_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ustruktureret data - et hurtigt overblik\n",
    "\n",
    "Vi starter med et hurtigt eksempel, der gennemg√•r et typisk workflow med tekst mining (med lidt web scraping blandet ind i):\n",
    "\n",
    "1. Tekst indl√¶ses (her forsiden af TV2 nyheder)\n",
    "2. Tekst udv√¶lges (her selve nyhedsoverskrifterne p√• forsiden)\n",
    "3. Tekst behandles (\"tokenization\")\n",
    "    - Fjern stopord\n",
    "    - Fjern tegns√¶tning\n",
    "    - Udvalg af bestemt ordtype\n",
    "4. Analyse foretages (her ordt√¶lling og ordsky-visualisering)\n",
    "\n",
    "Resten af lektionen bruges p√• at dykke ned i hvert af disse skridt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tekst indl√¶ses\n",
    "\n",
    "`requests` bruges til at sende en foresp√∏rgsel til nyheder.tv2.dk. HTML-indholdet af siden gemmes i et objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nyheder.tv2.dk/\"\n",
    "url_content = requests.get(url).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tekst udv√¶lges\n",
    "\n",
    "`beautifulsoup` bruges til at finde specifikke dele af HTML'en. Her nyhedsoverskrifterne. Nyhedsoverskrifterne s√¶ttes derefter sammen til √©n string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv2_soup = bs(url_content)\n",
    "texts = tv2_soup.find_all(\"a\", class_ = \"o-teaser_link\")\n",
    "texts = [text.get_text() for text in texts]\n",
    "text = ''.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tekst behandles (\"tokenization\")\n",
    "\n",
    "`spacy` bruges til at \"tokenize\" teksten; alts√• inddelen teksten i enkeltord, som er meningsfulde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "words = []\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN']\n",
    "stopwords = list(nlp.Defaults.stop_words) + [\"LIVE\"]\n",
    "punctuation = string.punctuation + \"‚Äù\"\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "        \n",
    "    if(token.pos_ in pos_tag):\n",
    "        words.append(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tekst analyseres\n",
    "\n",
    "Med teksten omdannet til tokens, kan de nu opt√¶lles (ved at konvertere dem til en pandas series) og visualiseres.\n",
    "\n",
    "### Ordt√¶lling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.Series(words).value_counts()\n",
    "counts = counts[counts > 1]\n",
    "\n",
    "counts.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordsky\n",
    "\n",
    "Ordene kan ogs√• visualiseres i ordsky med pakken `wordcloud`, som ogs√• accepterer pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\")\n",
    "wc.fit_words(counts)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data fra nettet - Web scraping\n",
    "\n",
    "## Hvad er web scraping?\n",
    "\n",
    "\"Web scraping\" er en paraplybetegnelse for automatiske teknikker til at indsamle information fra internettet (alts√• indsamling som ikke foreg√•r via en browser).\n",
    "\n",
    "At arbejde med web scraping involverer b√•de at indsamle r√•data fra internettet samt behandle og konvertere disse data til et format, som er til at arbejde med. \n",
    "\n",
    "## Hvorfor er web scraping relevant? (for samfundsvidenskaberne)\n",
    "\n",
    "**1. Internettet er i sig selv et relevant genstandsfelt.**\n",
    "\n",
    "Internettet er i dag fuldst√¶ndig integreret i vores hverdagsliv. B√•de information p√• internettet og vores brug af det giver indblik i vaner, forbrug, interaktion.\n",
    "\n",
    "**2. Internettet er en datakilde**\n",
    "\n",
    "Internettet er en stor samling af information. Vi har brug for teknikker til systematisk at indhente information derfra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hvad er internettet *egentlig*? \n",
    "\n",
    "For at kunne arbejde med internettet, er det rart at have en grundl√¶ggende forst√•else for, hvad internettet egentlig er for en st√∏rrelse. \n",
    "\n",
    "Herunder nogen v√¶sentlig punkter.\n",
    "\n",
    "- Internettet: Et global system af forbundede computere og servere.\n",
    "    - \"Internet\" forkortelse for \"interconnected network\"\n",
    "\n",
    "- \"(World Wide) Web\" (WWW): En samling af ressourcer, som er tilg√¶ngelige via internettet \n",
    "    - \"Webbet\" kan betragtes som de navigerbare og offentlige dele af internettet\n",
    "\n",
    "- Enhver computer eller server p√• internettet har et unik id i form af en IP-adresse (127.28.115.253)\n",
    "    - IP: Internet Protocol\n",
    "\n",
    "- N√•r man tilg√•r internettet via webbet, kontaktes andre computere og servere for at hente information.\n",
    "\n",
    "- P√• webbet bruges \"URL'er\" til at give unikke (og l√¶sbare) adresser til serverne.\n",
    "    - URL: Uniform Ressource Locators\n",
    "\n",
    "- Forbindelsen mellem IP-adresser og URL h√•ndteres af DNS servere\n",
    "    - DNS: Domain Name System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3f/Internet_map_1024_-_transparent%2C_inverted.png\" alt=\"opte\" width=\"600\"/>\n",
    "\n",
    "*Opte Project 2005*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S√•dan virker internettet (\"the web\") - Kort fortalt\n",
    "\n",
    "![dns](https://cdn2.wpbeginner.com/wp-content/uploads/2019/06/howdomainswork.png)\n",
    "\n",
    "*wpbeginner.com*\n",
    "\n",
    "**Konsekvens: Internettet har ikke en samlet adressebog**\n",
    "\n",
    "Fordi der er flere udbyder af DNS, dom√¶ner osv., er der ikke √©n samlet registrering af alle hjemmesider. S√∏gemaskiner som Google, DuckDuckGo og Bing skaber derfor deres egen \"telefonbog\" over internettet, s√• vi kan s√∏ge i det. \n",
    "\n",
    "De samme teknikker kan man selv genskabe i form af \"crawlers\" eller \"spiders\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internettet gennem din browser versus gennem Python\n",
    "\n",
    "### Internettet gennem din browser\n",
    "\n",
    "![browser_img](./img/website_browser.png)\n",
    "\n",
    "Ovenst√•ende er internettet, som I kender det (m√•ske nogen kender den del af internettet lidt for godt). Hvordan ser det ud gennem Python?\n",
    "\n",
    "### Internettet gennem Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.get(\"https://www.reddit.com/r/PrequelMemes/\").content[0:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sikke en rodebutik!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hvad betyder alt det her?\n",
    "\n",
    "Det er v√¶rd at v√¶re opm√¶rksom p√•, hvordan internettet egentlig h√¶nger sammen, hvis vi fors√∏ger at g√∏re noget med data fra det. Typisk er vores indgang til internettet gennem browsere eller apps, hvor alting er sat op s√•dan, at vi ikke beh√∏ver at forholde sig til disse ting.\n",
    "\n",
    "N√•r vi arbejder med data fra internettet, er det altid data, som ligger et eller andet sted (p√• en server) ofte i et format, der i sin r√• form ikke giver meget mening at arbejde med (r√• HTML).\n",
    "\n",
    "Der er derfor b√•de fejlkilder i adgangen til data (forbindelser mellem servere kan svigte - ogs√• bare kortvarigt) og i h√•ndteringen af det."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adgang til internettet via Python (`requests`)\n",
    "\n",
    "Pakken `requests` bruges til at sende henvendelser over internettet. Herunder sendes en s√•kaldt \"GET\" request til nyheder.tv2.dk (n√•r du bes√¶ger hjemmesider via en browser, sender din browser ogs√• GET requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://nyheder.tv2.dk/\"\n",
    "req = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kom vi igennem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(req.status_code), str(req.reason))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad indeholder siden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = req.content\n",
    "print(content[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det, som vi f√•r, er hjemmesidens \"r√•\" indhold; alts√• HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En hurtig introduktion til HTML\n",
    "\n",
    "HTML er et helt kodesprog i sig selv og kan tage lang tid at mestre. Vi beh√∏ver dog kun et overordnet kendskab til dens opbygning for at kunne navigere i det.\n",
    "\n",
    "- HTML best√•r af \"tags\" afgr√¶nset med `<>`\n",
    "- Tags er altid struktureret i en tr√¶-lignende hierarkisk struktur\n",
    "\n",
    "Herunder en kort HTML string som eksempel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div id=\"convo1\">\n",
    "            <p class=\"kenobi\">Hello There!</p>\n",
    "        </div>\n",
    "        <div id=\"convo2\">\n",
    "            <p class=\"grievous\">General Kenobi!</p>\n",
    "        </div>\n",
    "        <div id=\"convo3\">\n",
    "            <p class=\"kenobi\">So Uncivilized!</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Tags\n",
    "\n",
    "- Tags bruges til at indikere typen af indhold (p: paragraph, h1/h2/h3: heading 1/2/3, a: link, img: billede osv.)\n",
    "- Tags indeholder forskellige \"attributes\":\n",
    "    - Id'er: `<div id=\"div2\">`\n",
    "        - id's er for det meste unikke\n",
    "    - Classes: `<div class=\"class!\">`\n",
    "        - Classes er for det meste ikke unikke\n",
    "    - Andre eksempler: href (link), rel, type, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naviger HTML med BeautifulSoup\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "\"BeautifulSoup\" pakken er udviklet til at navigere i HTML. Den virker ved at omdanne html til et \"soup\"-objekt, hvor man kan pege p√• specifikke tags og udlede indhold.\n",
    "\n",
    "Herunder omdannes HTML-strengen fra f√∏r til et soup objekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "soup = bs(html, \"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det ligner det samme stykke tekst, men der kan nu navigeres efter de forskellige tags.\n",
    "\n",
    "Lad os starte med at se p√•, hvilke tags der er:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tag.name for tag in soup.find_all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at finde specifikke tags bruges metoderne `.find()` (f√∏rste match) og `.find_all()` (alle matches). F√∏rste argument er tagget, som man leder efter. Man kan derefter specificerer andre argumenter for at indsn√¶vre s√∏gningen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\") # Finder f√∏rste p tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"p\") # Finder alle p tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.get_text()` henter selve tekst-indholdet, som er inde i tagget (alts√• mellem `<p>` og `</p>`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\").get_text() # Tager teksten ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [] # Teksten for alle tags\n",
    "\n",
    "for tag in soup.find_all(\"p\"):\n",
    "    texts.append(tag.get_text())\n",
    "    \n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kan som n√¶vnt specificerer flere argumenter. `id = ` bruges til at finde et bestemt id, mens `class_ = ` bruges til at finde bestemt class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", id = \"convo2\").get_text() # Find ud fra bestemt id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", id = \"convo2\").get_text(strip = True) # Samme ovenst√•ende, men fjerner whitespace (fx \\n som indikerer ny linje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"p\", class_ = \"kenobi\").get_text() # Find ud fra bestemt klasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find de rigtige tags\n",
    "\n",
    "At finde de rigtige tags til at finde tekst ud fra kr√¶ver lidt forarbejde. Det nemmeste er ved at bruge \"Code Inspector\" funktionen, som findes i de fleste browsere. \n",
    "\n",
    "![inspector](./img/inspector_tv.png)\n",
    "\n",
    "HTML kan v√¶re meget rodet, s√• det involverer ofte at pr√∏ve sig lidt frem.\n",
    "\n",
    "Bem√¶rk fx at et tag kan indeholde flere klasser (adskilt med mellemrum). Dog beh√∏ver vi kun en af dem, for at matche det med BeautifulSoup.\n",
    "\n",
    "P√• forsiden af indlandssektionen p√• nyheder.tv2.dk kan vi se, at overskrifter befinder sig i et `<a>` tag med klassen: `o-teaser_link`.\n",
    "\n",
    "Denne klasse kan vi bruge til at matche overskrifterne.\n",
    "\n",
    "Vi har allerede HTML-indholdet fra nyheder.tv2.dk l√¶st ind som `content`, som vi kan konvertere til et soup-objekt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan derefter udlede overskrifter baseret p√• klassen.\n",
    "\n",
    "Herunder den f√∏rste overskrift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\", class_ = \"o-teaser_link\").get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan med et for loop udlede alle overskrifterne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(\"a\", class_ = \"o-teaser_link\"):\n",
    "    print(tag.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hvad med links?\n",
    "\n",
    "Ofte i web scraping kan det give mening at indsamle links. P√• den m√•de kan man s√¶tte \"crawlers\" op, der ...\n",
    "1. G√•r til en hjemmeside\n",
    "2. Indsamler indhold\n",
    "3. Indsamler links\n",
    "4. Indsamler indhold p√• hver side, som der linkes til\n",
    "5. (gentag)\n",
    "\n",
    "Links er altid med et `<a>` tag i en attribut `href`. Et tag kan have flere attributes (`id` og `class` er fx attributes). Med et soup objekt for et bestemt tag, kan man udlede et bestemt attribut med `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\", class_ = \"o-teaser_link\")['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der er i ovenst√•ende tale om et relativt link (et, der bygger videre p√• hovedadressen: https://nyheder.tv2.dk/), s√• vi kan f√¶rdigg√∏re linket ved at paste det sammen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://nyheder.tv2.dk\" + soup.find(\"a\", class_ = \"o-teaser_link\")['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvis vi ser p√• selve HTML indholdet af et tag for en overskrift kan vi se, at der ogs√• er et title attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"a\", class_ = \"o-teaser_link\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan derfor alternativt udlede overskrifter p√• nyheder.tv2.dk med f√∏lgende (som giver et lidt p√¶nere resultat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(\"a\", class_ = \"o-teaser_link\"):\n",
    "    print(tag['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √òVELSE: Basal web scraping\n",
    "\n",
    "Lad os pr√∏ve at skrive et program, der finder overskrifter p√• Berlingske (https://www.berlingske.dk/nyheder).\n",
    "\n",
    "1. Indl√¶s HTML-indholdet af forsiden af Berlingske med `requests.get(url).content` (husk at gemme det i et objekt)\n",
    "2. Konverter HTML-indholdet til et soup-objekt med `bs(content)` (husk at gemme det i et objekt)\n",
    "3. Print tekstindholdet af det f√∏rste `<h4>` tag (brug `soup.find()` og `.get_text()`)\n",
    "\n",
    "**BONUS**\n",
    "4. Skriv et stykke kode, der udleder linket til den f√∏rste artikel p√• https://www.berlingske.dk/nyheder (udforsk HTML enten direkte i python eller gennem din browser)\n",
    "\n",
    "## L√òSNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At arbejde med tekst data\n",
    "\n",
    "Lad os arbejde lidt videre med artikeloverskrifter fra nyheder.tv2.dk, men f√∏rst skal vi have lidt data at arbejde med.\n",
    "\n",
    "Vi opbygger herunder en dictionary, der indeholder artikeloverskrifter og links. Vi konverterer derefter denne dictionary til en pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://nyheder.tv2.dk/\"\n",
    "req = requests.get(url)\n",
    "content = req.content\n",
    "soup = bs(content)\n",
    "\n",
    "ber_dict = {}\n",
    "key = 0\n",
    "\n",
    "for tag in soup.find_all(\"a\", class_ = \"o-teaser_link\"):\n",
    "    article_dict = {}\n",
    "    article_dict[\"title\"] = tag['title']\n",
    "    article_dict[\"link\"] = tag['href']\n",
    "    ber_dict[key] = article_dict\n",
    "    key = key + 1\n",
    "    \n",
    "article_data = pd.DataFrame.from_dict(ber_dict, orient = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor mange artikler indeholder corona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['title'].str.contains(\"corona\", case = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor mange artikler indeholder Trump?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['title'].str.contains(\"trump\", case = False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tilf√∏jet som variabel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['corona'] = article_data['title'].str.contains(\"corona\", case = False)\n",
    "article_data['trump'] = article_data['title'].str.contains(\"trump\", case = False)\n",
    "\n",
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er der artikler om trump og corona?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.loc[(article_data['corona'] == True) & (article_data['trump'] == True), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = article_data.groupby(['corona', 'trump'])\n",
    "\n",
    "grouped_df.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √òVELSE: Indhold i tekst\n",
    "\n",
    "Datas√¶ttet 'pol-tweets-2020_sample.csv' (https://github.com/CALDISS-AAU/workshop_python-text-mining/raw/master/data/pol-tweets-2020_sample.csv) indholder et udtr√¶k af 500 tilf√¶ldige tweets fra danske politikere.\n",
    "\n",
    "1. Lav en variabel, der indikerer, om tweetet indeholder ordet \"klima\"\n",
    "2. Find ud af hvor mange tweets, der n√¶vner \"klima\"\n",
    "\n",
    "**BONUS**\n",
    "3. Skriv kode (fx en visualisering), der viser, hvem der snakker meget om klima\n",
    "\n",
    "## L√òSNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvem snakker om klima?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udledning af n√∏gleord\n",
    "\n",
    "Lad os vende tilbage til forsideartiklerne.\n",
    "\n",
    "- Hvad bliver der s√¶rligt diskuteret i dag?\n",
    "- Hvilke artikler behandler det mest diskuterede tema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "\"Tokenization\" er et typisk skridt i behandlingen af tekst data. Tokenization er processen, der omdanner r√• tekst til enkelte tekstenheder (tokens) - typisk i form af enkeltord.\n",
    "\n",
    "Typiske skridt i tokenization:\n",
    "\n",
    "1. Opdeling af tekst i enkeltord\n",
    "2. Frasortering af tegns√¶tning\n",
    "3. Evt. konverter til sm√• bogstaver\n",
    "4. Frasorter stopord\n",
    "5. Evt. konverter til \"stammen\" (stemming eller lemmatization)\n",
    "\n",
    "Der findes efterh√•nden mange v√¶rkt√∏jer der kan udf√∏re tokenization.\n",
    "\n",
    "F√∏rst tager vi skridt 1-4 med mere basale Python kommandoer. Derefter ser vi p√• den \"smarte\" l√∏sning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Opdeling af tekst i enkeltord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = \"F√∏rst ville Trump slet ikke forhandle, men nu vil han gerne lidt alligevel - og det koster point i valgkampen\"\n",
    "\n",
    "words = heading.split(\" \")\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Frasortering af tegns√¶tning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_list = [',', '-','.','?','!']\n",
    "\n",
    "words_nopunct = []\n",
    "\n",
    "for word in words:\n",
    "    for punct in punct_list:\n",
    "        word = word.replace(punct, \"\")\n",
    "    words_nopunct.append(word)\n",
    "    \n",
    "words_nopunct = list(filter(None, words_nopunct))\n",
    "words_nopunct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Konverter til sm√• bogstaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lower = [word.lower() for word in words_nopunct]\n",
    "words_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Frasorter stopord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['f√∏rst', 'ville', 'slet', 'ikke', 'men', 'nu', 'vil', 'han', 'gerne', 'lidt', 'alligevel', 'og', 'det', 'koster', 'i']\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for word in words_lower:\n",
    "    if word not in stopwords:\n",
    "        tokens.append(word)\n",
    "        \n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming / lemmatization\n",
    "\n",
    "Form√•let med tokenization er b√•de at frasortere st√∏j i teksten og ensrette tokens s√•dan, at der kun er √©t token for alle ord med samme semantiske betydning.\n",
    "\n",
    "Fx er det uhensigtsm√¶ssigt at behandle \"Valgkamp\" og \"valgkamp\" som to forskellige tokens, da det er det samme ord bare med stort og lille forbogstav.\n",
    "\n",
    "Netop fordi hvert ord som udgangspunkt behanldes som unikt, giver det nogen udfordring ift. grammatik, da hver ordb√∏jning bliver hver sit token - medmindre man g√∏r noget ved det!\n",
    "\n",
    "Derfor er en typisk praksis enten at konvertere ordet til stammen (stemming). Her ville ord som \"koste\", \"koster\", \"kostede\" alle konverteres til \"kost\", da det er ordets stamme.\n",
    "\n",
    "En anden praksis er at konvertere til navneformen (lemmatization). Her ville ord som \"koste\", \"koster\", \"kostede\" alle konverteres til \"koste\", da det er navneform for ordet.\n",
    "\n",
    "S√•danne konverteringer kr√¶ver, at man bruger i forvejen tr√¶nede sprogmodeller, da stamme og navneform vil variere fra sprog til sprog. Heldigvis er der udviklet mange af disse v√¶rkt√∏jer.\n",
    "\n",
    "Dog er s√•dan nogen v√¶rkt√∏jer ikke uden fejl, da det tager lang tid for en computer at l√¶re, hvad forskellen er p√• \"kost\" (noget man spiser), \"kost\" (noget man fejer med) og \"kost\" (stammen at verbum \"koste\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization med spacy\n",
    "\n",
    "Ovenst√•ende er en noget langsommelige proces. Lad os pr√∏ve med en tokenizer fra pakken \"spacy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"da_core_news_sm\") # Indl√¶sning af dansk sprogmodel\n",
    "\n",
    "doc = nlp(heading) # Behandling af tekststykke med sprogmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print af de umiddelbart udledte tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens lemmatiseret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvilken ordklasse er ordene? (her er der nogen fejl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print('{0:10.10} {1}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VIDENSCHECK**: Kan I spotte nogen fejlklassificeringer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med sprogmodellen i spacy f√∏lger i √∏vrigt en pr√¶defineret stopordsliste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nlp.Defaults.stop_words)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fra standardpakken `string` kan vi skaffe tegns√¶tning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med disse v√¶rkt√∏jer kan vi danne vores token-liste p√• f√∏lgende m√•de. Vi kan samtidig filtere efter bestemte ordklasser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab) # Indl√¶sning af tokenizer\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN'] # Beholder adjektiver og navneord\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "punctuation = string.punctuation + \"‚Äù\"\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    if(token.text.lower() in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "        \n",
    "    if(token.pos_ in pos_tag):\n",
    "        tokens.append(token.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tilbage st√•r vi (med denne sortering) med f√∏lgende meningsfulde tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udled n√∏gleord - liste tilgang\n",
    "\n",
    "Lad os nu kombinere ovenst√•ende til at udlede n√∏gleord af avisartiklerne. Vi kan samtidig filtere efter bestemte ordklasser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings = ' '.join(list(article_data['title'])) # Al tekst samles i en string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(headings)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab) # Indl√¶sning af tokenizer\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN'] # Beholder adjektiver og navneord\n",
    "stopwords = list(nlp.Defaults.stop_words)\n",
    "punctuation = string.punctuation + \"‚Äù\"\n",
    "\n",
    "for token in doc:\n",
    "    \n",
    "    if(token.text.lower() in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "        \n",
    "    if(token.pos_ in pos_tag):\n",
    "        tokens.append(token.text.lower())\n",
    "        \n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = pd.Series(tokens)\n",
    "\n",
    "counts = token_series.value_counts()\n",
    "counts = counts[counts > 1]\n",
    "\n",
    "counts.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og med ordsky:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color=\"white\")\n",
    "wc.fit_words(counts)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udled n√∏gleord - tidy data tilgang\n",
    "\n",
    "Ulempen ved ovenst√•ende tilgang er, at det kan blive vanskeligt at koble teksten tilbage til oprindelige data.\n",
    "\n",
    "En m√•de, at udf√∏re tekstanalyser, men hvor man samtidig holder data struktureret, er ved at f√• data √¶ndret p√• s√•dan en m√•de, at man kan koble ordbrug til bestemte tekster. \n",
    "\n",
    "Dette er et eksempel p√• en tidy data tilgang. Tidy data dikterer, at hver r√¶kke i et datas√¶t skal v√¶re en unik observation.\n",
    "Til arbejde med tekst opn√•r vi en del fleksibilitet ved at f√• et datas√¶t, hvor hver r√¶kke indeholder:\n",
    "- Teksten (fx en overskrift eller et tweet)\n",
    "- Baggrundsinformation om teksten (evt. dato eller navn p√• twitter-bruger)\n",
    "- Ordet i teksten\n",
    "\n",
    "Det giver hurtigt et meget stort datas√¶t, men det har mange fordele i det senere analyse arbejde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktion til at danne tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    tokenizer = Tokenizer(nlp.vocab) # Indl√¶sning af tokenizer\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] # Beholder adjektiver og navneord\n",
    "    stopwords = list(nlp.Defaults.stop_words)\n",
    "    punctuation = string.punctuation + \"‚Äù\"\n",
    "\n",
    "    for token in doc:\n",
    "    \n",
    "        if(token.text.lower() in stopwords or token.text in punctuation):\n",
    "            continue\n",
    "\n",
    "        if(token.pos_ in pos_tag):\n",
    "            tokens.append(token.text.lower())\n",
    "        \n",
    "    return(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funktionen bruges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['tokens'] = article_data['title'].apply(tokenize_function)\n",
    "\n",
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoden `.explode()` splitter v√¶rdier i en liste og danner en ny r√¶kke for hver v√¶rdi. P√• den m√•de opn√•s et tidy data s√¶t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = article_data.explode(column = 'tokens')\n",
    "\n",
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data er nu klar til at foretage forskellige opt√¶llinger. \n",
    "\n",
    "Hyppigste tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['tokens'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T√¶lling af tokens per titel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data.groupby(['title', 'tokens']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hotte\" emner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lad os se, om vi kan udlede hvilke artikler, der skriver om dagens popul√¶re emner (ud fra overskrift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_topics = list(article_data['tokens'].value_counts().index[0:5])\n",
    "hot_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nedenst√•ende danner logiske variabel \"hot_topic\". Den er dannet ved at g√• igennem hvert ord i `hot_topics` og tjekke, om titlen indeholder dette ord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data['hot_topic'] = False\n",
    "\n",
    "for topic in hot_topics:\n",
    "    article_data.loc[article_data['hot_topic'] != True, 'hot_topic'] = article_data.loc[article_data['hot_topic'] == False, 'title'].str.contains(topic)\n",
    "\n",
    "article_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan derefter visualisere de hyppigste ord for de artikler, som omhandler de popul√¶re emner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_count = article_data.loc[article_data['hot_topic'] == True, :]['tokens'].value_counts()\n",
    "\n",
    "wc = WordCloud(background_color=\"white\")\n",
    "wc.fit_words(hot_count)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √òVELSE: N√∏gleord fra tweets\n",
    "\n",
    "Udled n√∏gleord af datas√¶ttet \"pol-tweets-2020_sample\" (link).\n",
    "\n",
    "Unders√∏g hvem der s√¶rligt tweeter om n√∏gleordene.\n",
    "\n",
    "\n",
    "## L√òSNING - Liste tilgang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L√òSNING - tidy data tilgang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L√òSNING - BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √òVELSE: N√∏gleord fra overskrifter\n",
    "\n",
    "Udled n√∏gleord (mest popul√¶re tokens) af overskrifterne p√• Berlingske (https://www.berlingske.dk/nyheder)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
